
import { Document, PipelineStep } from './types';

export const PRIVATE_DOCUMENT: Document = {
  id: 'doc-1',
  name: 'University AI Policy 2025.pdf',
  type: 'Policy',
  content: `
    UNIVERSITY OF EXCELLENCE: 2025 AI ETHICS & USAGE POLICY
    Effective Date: January 1, 2025
    
    1. GRADING GUIDELINES:
    Professors are permitted to use AI for initial drafting of feedback but MUST manually review all grades. Automatic grading without human oversight is strictly prohibited.
    
    2. ATTENDANCE TRACKING:
    The university has implemented a new 'Smart-Scan' biometric attendance system. Students must use their mobile app to scan a dynamic QR code generated by the professor at the start of each lecture.
    
    3. RESEARCH GRANTS:
    A new 'AI Innovation Grant' of $5,000 is available for faculty members starting projects that integrate Large Language Models into non-STEM curriculum.
    
    4. DATA PRIVACY:
    Student data must not be uploaded to public cloud-based LLMs without anonymization. Internal servers 'EXCEL-AI-CORE' should be used for sensitive records.
    
    5. CONTACT:
    For policy clarification, contact Dr. Sarah Chen, Dean of Digital Transformation at s.chen@excellence.edu.
  `
};

export const SAMPLE_QUESTIONS = [
  "What is the specific grant amount for faculty integrating AI into non-STEM fields?",
  "How does the new attendance system work for students in 2025?",
  "Who should I contact for AI policy clarifications at the University of Excellence?"
];

export const PIPELINE_FLOW = [
  PipelineStep.INGEST,
  PipelineStep.CLEAN,
  PipelineStep.CHUNK,
  PipelineStep.EMBED,
  PipelineStep.STORE,
  PipelineStep.QUERY_EMBED,
  PipelineStep.RETRIEVAL,
  PipelineStep.RERANK,
  PipelineStep.PROMPT,
  PipelineStep.GENERATE
];

export const STEP_METADATA: Record<PipelineStep, { title: string; description: string; tools: string[] }> = {
  [PipelineStep.QUESTION]: {
    title: "User Question",
    description: "The journey begins with a natural language query from the user.",
    tools: ["React Forms", "User Input"]
  },
  [PipelineStep.INGEST]: {
    title: "Document Ingestion",
    description: "Collecting private or recent organization-specific data that the LLM was not trained on.",
    tools: ["PDFPlumber", "PyPDF2", "Unstructured.io"]
  },
  [PipelineStep.CLEAN]: {
    title: "Text Extraction & Cleaning",
    description: "Removing headers, footers, and noise to ensure the LLM receives high-quality information.",
    tools: ["BeautifulSoup", "Regular Expressions", "LangChain Document Loaders"]
  },
  [PipelineStep.CHUNK]: {
    title: "Semantic Chunking",
    description: "Breaking long documents into smaller, manageable pieces to fit into the LLM's limited memory (Context Window).",
    tools: ["LangChain RecursiveCharacterTextSplitter", "LlamaIndex SentenceSplitter"]
  },
  [PipelineStep.EMBED]: {
    title: "Embedding Generation",
    description: "Converting text chunks into numerical vectors (lists of numbers) that represent 'meaning'.",
    tools: ["OpenAI Text-Embedding-3", "Google Vertex AI", "HuggingFace Sentence-Transformers"]
  },
  [PipelineStep.STORE]: {
    title: "Vector Store / Indexing",
    description: "Storing vectors and their corresponding text in a specialized database for lightning-fast similarity search.",
    tools: ["Pinecone", "ChromaDB", "FAISS", "Weaviate", "pgvector"]
  },
  [PipelineStep.QUERY_EMBED]: {
    title: "Query Embedding",
    description: "The user's question is also converted into a vector using the SAME embedding model.",
    tools: ["Same Embedding Provider"]
  },
  [PipelineStep.RETRIEVAL]: {
    title: "Vector Retrieval",
    description: "Finding the top-K chunks whose vectors are mathematically closest to the question's vector.",
    tools: ["Cosine Similarity", "Euclidean Distance"]
  },
  [PipelineStep.RERANK]: {
    title: "Optional Reranking",
    description: "Using a more powerful (but slower) model to re-order the retrieved results for maximum relevance.",
    tools: ["Cohere Rerank", "BGE-Reranker"]
  },
  [PipelineStep.PROMPT]: {
    title: "Prompt Assembly",
    description: "The 'Secret Sauce': Combining the question with the retrieved chunks and specific rules for the LLM.",
    tools: ["Prompt Templates", "System Messages"]
  },
  [PipelineStep.GENERATE]: {
    title: "Final Generation",
    description: "The LLM generates an answer grounded specifically in the provided context, reducing hallucinations.",
    tools: ["Gemini 2.5/3", "GPT-4", "Claude 3.5"]
  },
  [PipelineStep.COMPARE]: {
    title: "Comparison Result",
    description: "Analyze the difference between standard LLM response and the RAG-enhanced response.",
    tools: ["Human Evaluation"]
  }
};
